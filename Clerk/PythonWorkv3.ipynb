{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Code Style\n",
    "# pip install BeautifulSoup as b\n",
    "from bs4 import BeautifulSoup as b\n",
    "#pip install requests\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "\n",
    "myURL= 'https://www.fpds.gov/ezsearch/fpdsportal?indexName=awardfull&templateName=1.5.2&s=FPDS.GOV&q=+SIGNED_DATE%3A%5B2019%2F10%2F30%2C2022%2F02%2F28%5D&x=24&y=7'\n",
    "downloadURL = r.get(myURL,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "\n",
    "#fullTable = soup.select('<span class=\"results_text\">')[0]\n",
    "#driver=fullTable.select('a span')\n",
    "\n",
    "#raw tags\n",
    "fulltable_findall= soup.find_all('<span class=\"results_text\">')\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_text')\n",
    "\n",
    "#driver = fulltable.find_all('a span class=')\n",
    "\n",
    "#tableRows = []\n",
    "\n",
    "#for d in driver:\n",
    "  #  tableRows.append(str(d).replace('<span class=\"standing-table_cell--name-text\">','').replace('</span>','').strip())\n",
    "\n",
    "#df = p.DataFrame(tableRows, columns=[ContractID])\n",
    "\n",
    "\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assisted Code Style\n",
    "datalist = []\n",
    "slop =[]\n",
    "for d in fulltable_select: \n",
    "    try: \n",
    "        datalist.append(d.a.contents)\n",
    "    except:\n",
    "        try:\n",
    "            datalist.append(d.span.contents)\n",
    "        except:\n",
    "            #formerly known as slop\n",
    "            datalist.append(str(d.contents).replace(\"\\\\t\",\"\").replace(\"\\\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'\",\"\"))\n",
    "            continue\n",
    "newlist = [\"\".join(d) if type(d) is list else d for d in datalist ]\n",
    "dlarr = np.array(newlist).reshape(30,19)\n",
    "#[\",\".join(d) if type(d) is list else   arr in dlarr]\n",
    "df = p.DataFrame(data= dlarr\n",
    ", columns=['AwardID','AwardType','LegalBusinessName','ContractingAgency','DateSigned','ActionObligation','ReferencedIDV','ContractingOffice','NAICS_Code','PSC_Code','EntityCity','UniqueEntityID_DUNS', 'EntityState','UniqueEntityID_SAM','Zip','UltimateParentUniqueEntityID_DUNS','UltimateParentLegalBusinessName','UltimateParentUniqueEntityID_SAM','CAGE_Code']\n",
    ")\n",
    "df.to_csv('slop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requires Fix fpr import date\n",
    "# \n",
    "# #pip install BeautifulSoup as b  & #pip install requests\n",
    "from bs4 import BeautifulSoup as b\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "from datetime import date, timedelta\n",
    "#import datetime as datetime\n",
    "#import calendar as timedelta\n",
    "\n",
    "# getting search days from user\n",
    "startDate = input('Enter the search start date in YYYY-MM-DD format')\n",
    "Syear, Smonth, Sday = map(int, startDate.split('-'))\n",
    "date1 = datetime.date(Syear, Smonth, Sday)\n",
    "#print(date1)\n",
    "endDate = input('Enter the search end date in YYYY-MM-DD format')\n",
    "Eyear, Emonth, Eday = map(int, endDate.split('-'))\n",
    "date2 = datetime.date(Eyear, Emonth, Eday)\n",
    "#print(date2)\n",
    "\n",
    "#Finding days in the start year ***NLR****\n",
    "    # year=date1.year\n",
    "    # days=366 if calendar.isleap(year) else 365\n",
    "    # print(days) \n",
    "\n",
    "#Get new search string for each day\n",
    "stringA = \"https://www.fpds.gov/ezsearch/fpdsportal?q=+SIGNED_DATE%3A%5B\"\n",
    "base = date1\n",
    "while base <= date2:\n",
    "    endt = base + timedelta(days=1)\n",
    "    stringB = str(base.year)+\"%2F\"+base.strftime(\"%m\") + \"%2F\" + base.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=24&y=7&start=\"\n",
    "    myURLsearch = (stringA + stringB)\n",
    "    base = base + timedelta(days=1)\n",
    "\n",
    "#get first url search and find the last pagination page  for 30 records per page\n",
    "myURLsearch\n",
    "downloadURL = r.get(myURLsearch,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "#raw tags for pagination report\n",
    "fulltable_findall= soup.find_all('<span class=\"results_heading\"><b>1</b> - <b>')\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_heading')\n",
    "# for rTns in fulltable_select: \n",
    "#     mylen = str(rTns).replace('Results','').replace('<b>1</b>','').replace('-','').replace('<b>30</b>','').replace('of','').replace('<b>','').split('\\n')\n",
    "#     mylen = str(mylen).replace('</b>','').replace(', ','').replace('<span class=\"results_heading\">','').strip()\n",
    "#     mylen= ''.join(mylen.split())\n",
    "#     tableRows = []\n",
    "#     tableRows.append(mylen)\n",
    "#     df = p.DataFrame(tableRows,['rcdAmount'])\n",
    "#     print(df)\n",
    "#for rTns in fulltable_select: print(str(rTns).replace('          Results      <b>1</b>  -  <b>30</b> of <b>',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019%2F01%2F01%2C2019%2F01%2F02%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=24&y=7&start=1\n",
      "644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Working Copy  2022-02-25 0900AM\n",
    "#pip install BeautifulSoup as b  & #pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup as b\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import pandas as p\n",
    "import math as m\n",
    "from datetime import date, timedelta\n",
    "#import datetime as dt\n",
    "#import calendar\n",
    "\n",
    "# getting search days from user\n",
    "startDate = input('Enter the search start date in YYYY-MM-DD format')\n",
    "Syear, Smonth, Sday = map(int, startDate.split('-'))\n",
    "date1 = datetime.date(Syear, Smonth, Sday)\n",
    "#print(date1)\n",
    "endDate = input('Enter the search end date in YYYY-MM-DD format')\n",
    "Eyear, Emonth, Eday = map(int, endDate.split('-'))\n",
    "date2 = datetime.date(Eyear, Emonth, Eday)\n",
    "#print(date2)\n",
    "\n",
    "#Finding days in the start year ***NLR****\n",
    "    # year=date1.year\n",
    "    # days=366 if calendar.isleap(year) else 365\n",
    "    # print(days) \n",
    "\n",
    "#Get new search string for each day\n",
    "stringA = \"https://www.fpds.gov/ezsearch/fpdsportal?q=+SIGNED_DATE%3A%5B\"\n",
    "base = date1\n",
    "while base <= date2:\n",
    "    endx = date1\n",
    "    endt = date1 + timedelta(days=1)\n",
    "    stringB = str(endx.year)+\"%2F\"+endx.strftime(\"%m\") + \"%2F\" + endx.strftime(\"%d\") + \"%2C\"+str(endt.year)+\"%2F\" + endt.strftime(\"%m\") + \"%2F\" +endt.strftime(\"%d\") + \"%5D&s=FPDS.GOV&templateName=1.5.2&indexName=awardfull&x=24&y=7&start=1\"\n",
    "    myURLsearch = (stringA + stringB)\n",
    "    base = base + timedelta(days=1)\n",
    "    endx = endt\n",
    "print(stringB)\n",
    "#get first url search and find the last pagination page  for 30 records per page\n",
    "myURLsearch\n",
    "downloadURL = r.get(myURLsearch,headers={})\n",
    "soup = b(downloadURL.text,\"html.parser\")\n",
    "#CSS\n",
    "fulltable_select  = soup.select('span.results_heading')\n",
    "myLen = m.ceil(int(fulltable_select[1].find_all('b')[2].text)/30)\n",
    "print(myLen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<b>1</b>, <b>30</b>, <b>19311</b>]\n"
     ]
    }
   ],
   "source": [
    "for f in fulltable_select: print( f.find_all('b'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19311"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "int(fulltable_select[1].find_all('b')[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "glop =[]\n",
    "for df in fulltable_select: \n",
    "    dl = datalist.append(str(df.contents).replace('<span class=\"results_heading\">','').replace('Results','').replace(',','').replace('<b>','').replace('</b>','').replace('-','').replace('of','').replace('                ','').replace(' 1 ',''))\n",
    "zf = dl\n",
    "kz = repr(zf)  # Remove Next Soft List\n",
    "\n",
    "print(zk) \n",
    "print(zk[220:zLen])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a82ef6f466d63b27a58f5b9b3bce23153a40d6708f87a46e08961f1e808d4b51"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
